import { Card } from '@fusuma/client';

<!-- sectionTitle: 5. Trust, but Verify -->
<!-- note
- Next and I might rub a few developers off the wrong way here is

- trust by verify developers because...
-->
### ğŸ›‚
## Trust, but Verify (Developers)

--- 
<!-- note
- developers are not perfect and our performance intuition are often wrong, myself include.

- Many times I have suspected an performance issue in a part of the system only to realise the root cause is else where.

- This is especially true when the system we are building have multiple moving parts
-->
## Developer's performance intuitions are often wrong

Myself included

---

<!-- note
- here are some things devs might say 

- more ram/cpu - while this may work what some developer may not know is that the performane gain is not linear, going from 4gb to 8gb does not give you 2x the load

- same thing with adding more instances, going from 2 to 4 instances does not double the throughput, infact sometime it introduces bottleneck further down the stack for example in the database

- other things we say are its slow but it hasn't crashed yet, and my personal favourite your test is wrong.
-->
## Things Devs might say

- "Get a bigger instance with more ram/cpu"
- "Just add more instances" 
- "It must be the database"
- "Non-issue, it hasn't crashed yet"
- "Must be an environment issue"
- "Your tests are wrong"

--- 
<!-- note
- So this here is one of the earlier test results from a single node in our test cluster. We had about 6 running at the same time.

- While the test result looks ok, it was no where near what the developers had initially observed.

- Just take a note here of the 75th and 99th percentile roughly 28ms and 134 ms
-->
<img src="../static/images/us-east-result.png" alt="Performance Test Result" />

---

## ğŸ§
<!-- note
- and the developers had 0.1 ms at the 75th percentile and 68 ms at the 99th percentile

- the difference is night and day 

- So we had a few back and forth with the devs and I knew something was wrong with the devs results. It looked way too good to be true.
-->
```text
1% tile: 974.0 (ns)
5% tile: 1075.0 (ns)
10% tile: 2292.0 (ns)
25% tile: 2695.0 (ns)
50% tile: 3671.0 (ns)
75% tile: 10440.0 (ns)
90% tile: 10091923.799999999 (ns)
99% tile: 68835579.60000025 (ns)
```

---

<!-- note
- It turns out this is how they capture their performance metrics. 

- By measuring the time taken to submit the order in the actual code.

- You might think this is funny, but its definitely not the first time I have seen this. In fact it happens quite often. Most developers are scrambling to meet the deadline.
 
- building out some sort of external performance test is probably not very high on their priority list and they resort to shortcuts or they just don't have experience in performance testing.... full stop.
-->
## Things Devs might do

```rust
let start = Instant::now();
submit_order();
let duration = start.elapsed();

println!("Time elapsed in submit_order() is: {:?}", duration);
```

---
<!--note
- The problem I described is actually called the coordinated omission problem, the developer is unknowingly measuring the wrong metric. In this case they are only measuring the service time and not taking the entire picture into account.

- Using a coffee shop as an simpler example, the service time is how long a barista takes to make the coffee, like how the develper have use the loggers to measure the time. Its omitting the time a customer is spent waiting in line and also making an order with the cashier or in our case the response time.

- This is not the only way where you can have coordinated omission problem, in fact you may observe this very problem in your very own load runner. Some test runners reuses threads to send out requests which means that the requests are not sent at a constant rate but at a constant time rate. Meaning the test runner will only sent a request after it has receive the response for the previous request. Gatling and WRK2 are some tools out there that doesn't exhibit this issue.

- Gil Tene has an excellent talk on this very issue called How not to measure latency, I'd recommend you all to check it out.
-->
<Card
  right={<img src="../static/images/coffee_shop.jpg" alt="People Buying Coffee" />}
  left={
  <>
    <h2>Coordinated Omission Problem</h2>
    <p />
    <small>additional reading: <a href="https://www.youtube.com/watch?v=lJ8ydIuPFeU">"How NOT to Measure Latency" by Gil Tene</a></small>
  </>
  }
/>

---
<!-- note
- Observability is another way to measure performance issues and assist developers in analysing performance metrics without any bias or any work for that matter. 

- Using Application Performance Monitoring tools like new relic, data dog and dynatrace. youre able to capture huge amount of data on how well your application is performing without manually adding them to your code.
 
- and if you use it in tandem with log aggregators and correlation ids, you able to build a holistic view of your system to slice and dice however you like when a performance issue do occur.
-->
## Application Performance Monitoring Metrics
#### +
## Log Aggregation w/ Correlation IDs
#### =
## Observability 

--- 
<!-- note
- Performance testing is not a once off exercises, chances are you will be performing a few rounds of testings. 

- The data you collect each time from both a client and a server perspective would go a long way in identifying the bottleneck and assist the dev in resolving the issue. 

- Fixing an issue in one place may expose another bottleneck further down the stack. The more data you collect the more knowledge you would have of your system.
-->
### â± Collect Data

### ğŸ¾ Find the bottleneck

### ğŸ›  Fix it

### â™» Repeat 

---
<!-- note
- and lets just remind ourselves of this great quote again

- Without data you're just another person with an opinion.  
-->
## Without data you're just another person with an opinion. 

### - W. Edwards Deming
